{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7094592,"sourceType":"datasetVersion","datasetId":4088704}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport timm\nimport sys\n\n\n\nprint('System Version:', sys.version)\nprint('PyTorch version', torch.__version__)\nprint('Torchvision version', torchvision.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T14:26:47.947755Z","iopub.execute_input":"2023-12-03T14:26:47.948537Z","iopub.status.idle":"2023-12-03T14:26:56.497217Z","shell.execute_reply.started":"2023-12-03T14:26:47.948492Z","shell.execute_reply":"2023-12-03T14:26:56.496258Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"System Version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\nPyTorch version 2.0.0\nTorchvision version 0.15.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Began by importing the torch libraries as well as timm. timm has prepackaged tools that will be used for image classification in the pytorch model. After this, I created a pytorch dataset by inheriting from the Dataset class. I added two methods, len and getitem that I test in the two cells following the class. Additionally, I add a transform to resize the images in my data directory to a standard size of 128,128x3 since the model requires a standard image size. ","metadata":{}},{"cell_type":"code","source":"#creating an iteratable object that we can loop over to train data\nclass VirusDataset(Dataset):\n    def __init__(self, data_dir, transform = None):\n        #ImageFolder class from torchvision package and provide it with the data directory as well as the transform it will be given\n        self.data = ImageFolder(data_dir, transform = transform)\n\n    #pytorch method that tells data loader how many examples are in a dataset\n    def __len__(self):\n        return len(self.data)\n\n    #pytorch method that takes an index in dataset and returns one item    \n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    #I could add another method for classes that returns data classes from ImageFolder if I wanted to visualize the model results\n    #This would retrieve class names i.e CoronaVirus so I could display them on an axis ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.499054Z","iopub.execute_input":"2023-12-03T14:26:56.499641Z","iopub.status.idle":"2023-12-03T14:26:56.506672Z","shell.execute_reply.started":"2023-12-03T14:26:56.499605Z","shell.execute_reply":"2023-12-03T14:26:56.505575Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Testing the len method in the class VirusDataset\ndataset = VirusDataset(data_dir = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/train')\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.507893Z","iopub.execute_input":"2023-12-03T14:26:56.508501Z","iopub.status.idle":"2023-12-03T14:26:56.599938Z","shell.execute_reply.started":"2023-12-03T14:26:56.508464Z","shell.execute_reply":"2023-12-03T14:26:56.599159Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"code","source":"#Testing the getitem method in the class VirusDataset\nimage, label = dataset[16]\nprint(label)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.602259Z","iopub.execute_input":"2023-12-03T14:26:56.602980Z","iopub.status.idle":"2023-12-03T14:26:56.629658Z","shell.execute_reply.started":"2023-12-03T14:26:56.602948Z","shell.execute_reply":"2023-12-03T14:26:56.628834Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/train'\n#create disctionary target_to_class that associates given number with proper label\n#in the cell above, dataset[16] gives 3, which would be in the Rhinovirus\n#This makes sense because each training dataset has 5 images, so 0 = 0-4, 1 = 5-9...etc\ntarget_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\nprint(target_to_class)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.630794Z","iopub.execute_input":"2023-12-03T14:26:56.631397Z","iopub.status.idle":"2023-12-03T14:26:56.641685Z","shell.execute_reply.started":"2023-12-03T14:26:56.631366Z","shell.execute_reply":"2023-12-03T14:26:56.640736Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{0: 'Coronavirus', 1: 'Herpesvirus', 2: 'Influenzavirus', 3: 'Rhinovirus'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#convert image to a standard size of 128x128x3 (3 = RGB channels) and convert to pytorch tensor\ntransform = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\ndata_dir = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/train'\ndataset = VirusDataset(data_dir, transform)\n#check size for a random image\nimage, label = dataset[15]\nimage.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.642624Z","iopub.execute_input":"2023-12-03T14:26:56.642895Z","iopub.status.idle":"2023-12-03T14:26:56.763317Z","shell.execute_reply.started":"2023-12-03T14:26:56.642853Z","shell.execute_reply":"2023-12-03T14:26:56.762530Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 128, 128])"},"metadata":{}}]},{"cell_type":"markdown","source":"Next I wrap the created dataset with a pytorch dataloader that will process the images in the dataset. This step will batch the data when it is given to the actual model, especially helpful for large datasets (not the case with my test data)","metadata":{}},{"cell_type":"code","source":"#call pytorch dataloader and provide dataset\n#add a shuffle since objective is training data, will not be shuffling for test or validation\ndataloader = DataLoader(dataset, batch_size = 10, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.764413Z","iopub.execute_input":"2023-12-03T14:26:56.764672Z","iopub.status.idle":"2023-12-03T14:26:56.771231Z","shell.execute_reply.started":"2023-12-03T14:26:56.764650Z","shell.execute_reply":"2023-12-03T14:26:56.770530Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#test dataloader to see images and labels for a batch\nfor images, labels in dataloader:\n    break\nlabels, images\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.772458Z","iopub.execute_input":"2023-12-03T14:26:56.772996Z","iopub.status.idle":"2023-12-03T14:26:56.962850Z","shell.execute_reply.started":"2023-12-03T14:26:56.772965Z","shell.execute_reply":"2023-12-03T14:26:56.962056Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(tensor([2, 1, 2, 3, 3, 0, 3, 0, 0, 1]),\n tensor([[[[0.0000, 0.0000, 0.0039,  ..., 0.0353, 0.0392, 0.0353],\n           [0.0157, 0.0118, 0.0157,  ..., 0.0431, 0.0471, 0.0431],\n           [0.0353, 0.0275, 0.0235,  ..., 0.0549, 0.0549, 0.0510],\n           ...,\n           [0.0392, 0.0588, 0.0667,  ..., 0.0314, 0.0078, 0.0157],\n           [0.0353, 0.0588, 0.0627,  ..., 0.0157, 0.0314, 0.0157],\n           [0.0235, 0.0510, 0.0549,  ..., 0.0235, 0.0196, 0.0157]],\n \n          [[0.1020, 0.1098, 0.1412,  ..., 0.3725, 0.3529, 0.3294],\n           [0.1216, 0.1255, 0.1529,  ..., 0.3725, 0.3529, 0.3294],\n           [0.1373, 0.1373, 0.1608,  ..., 0.3765, 0.3569, 0.3333],\n           ...,\n           [0.4157, 0.4706, 0.5137,  ..., 0.0980, 0.0706, 0.0824],\n           [0.4118, 0.4706, 0.5098,  ..., 0.0824, 0.0980, 0.0824],\n           [0.4000, 0.4627, 0.5020,  ..., 0.0863, 0.0863, 0.0784]],\n \n          [[0.1686, 0.1725, 0.1961,  ..., 0.4510, 0.4353, 0.4196],\n           [0.1882, 0.1882, 0.2078,  ..., 0.4549, 0.4392, 0.4235],\n           [0.2000, 0.1961, 0.2118,  ..., 0.4588, 0.4431, 0.4275],\n           ...,\n           [0.5098, 0.5569, 0.5922,  ..., 0.1373, 0.1098, 0.1216],\n           [0.5059, 0.5569, 0.5882,  ..., 0.1216, 0.1373, 0.1216],\n           [0.4941, 0.5490, 0.5804,  ..., 0.1255, 0.1255, 0.1176]]],\n \n \n         [[[0.8157, 0.8157, 0.8157,  ..., 0.6353, 0.6392, 0.6510],\n           [0.8157, 0.8157, 0.8157,  ..., 0.6667, 0.6667, 0.6627],\n           [0.8157, 0.8157, 0.8157,  ..., 0.6941, 0.6863, 0.6706],\n           ...,\n           [0.6784, 0.6863, 0.6902,  ..., 0.6588, 0.6667, 0.6706],\n           [0.6824, 0.6863, 0.6902,  ..., 0.6588, 0.6627, 0.6706],\n           [0.6863, 0.6902, 0.6941,  ..., 0.6549, 0.6627, 0.6706]],\n \n          [[0.9569, 0.9569, 0.9569,  ..., 0.8745, 0.8745, 0.8863],\n           [0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8980, 0.8941],\n           [0.9569, 0.9569, 0.9569,  ..., 0.9059, 0.9059, 0.8941],\n           ...,\n           [0.9137, 0.9216, 0.9255,  ..., 0.8784, 0.8824, 0.8902],\n           [0.9176, 0.9216, 0.9255,  ..., 0.8745, 0.8784, 0.8902],\n           [0.9216, 0.9255, 0.9294,  ..., 0.8745, 0.8784, 0.8902]],\n \n          [[0.9490, 0.9490, 0.9490,  ..., 0.8627, 0.8784, 0.8863],\n           [0.9490, 0.9490, 0.9490,  ..., 0.8784, 0.8902, 0.8902],\n           [0.9490, 0.9490, 0.9490,  ..., 0.8706, 0.8863, 0.8745],\n           ...,\n           [0.9059, 0.9137, 0.9176,  ..., 0.8471, 0.8667, 0.8824],\n           [0.9098, 0.9137, 0.9176,  ..., 0.8431, 0.8588, 0.8784],\n           [0.9137, 0.9176, 0.9216,  ..., 0.8392, 0.8588, 0.8784]]],\n \n \n         [[[0.9608, 0.9647, 0.9686,  ..., 0.6000, 0.5961, 0.5961],\n           [0.9608, 0.9608, 0.9647,  ..., 0.6000, 0.5961, 0.5961],\n           [0.9569, 0.9608, 0.9647,  ..., 0.6000, 0.5961, 0.5961],\n           ...,\n           [0.6392, 0.6392, 0.6392,  ..., 0.5529, 0.5529, 0.5529],\n           [0.6353, 0.6353, 0.6353,  ..., 0.5529, 0.5529, 0.5529],\n           [0.6353, 0.6353, 0.6353,  ..., 0.5529, 0.5529, 0.5529]],\n \n          [[0.9647, 0.9686, 0.9725,  ..., 0.6824, 0.6784, 0.6784],\n           [0.9647, 0.9647, 0.9686,  ..., 0.6824, 0.6784, 0.6784],\n           [0.9608, 0.9647, 0.9686,  ..., 0.6824, 0.6784, 0.6784],\n           ...,\n           [0.7137, 0.7137, 0.7137,  ..., 0.6431, 0.6431, 0.6431],\n           [0.7098, 0.7098, 0.7098,  ..., 0.6431, 0.6431, 0.6431],\n           [0.7098, 0.7098, 0.7098,  ..., 0.6431, 0.6431, 0.6431]],\n \n          [[0.9725, 0.9765, 0.9804,  ..., 0.7020, 0.6980, 0.6980],\n           [0.9725, 0.9725, 0.9765,  ..., 0.7020, 0.6980, 0.6980],\n           [0.9686, 0.9725, 0.9765,  ..., 0.7020, 0.6980, 0.6980],\n           ...,\n           [0.7373, 0.7373, 0.7373,  ..., 0.6745, 0.6745, 0.6745],\n           [0.7333, 0.7333, 0.7333,  ..., 0.6745, 0.6745, 0.6745],\n           [0.7333, 0.7333, 0.7333,  ..., 0.6745, 0.6745, 0.6745]]],\n \n \n         ...,\n \n \n         [[[0.0627, 0.0627, 0.0627,  ..., 0.1451, 0.1686, 0.1882],\n           [0.0588, 0.0588, 0.0588,  ..., 0.1216, 0.1490, 0.1725],\n           [0.0549, 0.0549, 0.0588,  ..., 0.0941, 0.1216, 0.1451],\n           ...,\n           [0.1725, 0.1686, 0.1529,  ..., 0.1647, 0.1490, 0.1569],\n           [0.1569, 0.1686, 0.1647,  ..., 0.1529, 0.1529, 0.1608],\n           [0.1647, 0.1647, 0.1608,  ..., 0.1569, 0.1647, 0.1569]],\n \n          [[0.0039, 0.0039, 0.0039,  ..., 0.0196, 0.0157, 0.0078],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0118, 0.0157],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0039, 0.0118, 0.0196],\n           ...,\n           [0.1882, 0.1843, 0.1686,  ..., 0.1922, 0.1765, 0.1843],\n           [0.1725, 0.1843, 0.1804,  ..., 0.1804, 0.1804, 0.1882],\n           [0.1804, 0.1804, 0.1765,  ..., 0.1843, 0.1922, 0.1843]],\n \n          [[0.0235, 0.0235, 0.0196,  ..., 0.0275, 0.0235, 0.0196],\n           [0.0235, 0.0235, 0.0196,  ..., 0.0196, 0.0275, 0.0314],\n           [0.0196, 0.0196, 0.0196,  ..., 0.0157, 0.0275, 0.0353],\n           ...,\n           [0.2314, 0.2275, 0.2118,  ..., 0.2314, 0.2157, 0.2235],\n           [0.2157, 0.2275, 0.2235,  ..., 0.2196, 0.2196, 0.2275],\n           [0.2235, 0.2235, 0.2196,  ..., 0.2235, 0.2314, 0.2235]]],\n \n \n         [[[0.7725, 0.8314, 0.8510,  ..., 0.7020, 0.6000, 0.4510],\n           [0.7686, 0.8196, 0.8627,  ..., 0.7294, 0.4902, 0.4039],\n           [0.7843, 0.8275, 0.8824,  ..., 0.6627, 0.3922, 0.2627],\n           ...,\n           [0.1569, 0.1804, 0.1451,  ..., 0.1647, 0.1490, 0.1529],\n           [0.1686, 0.1529, 0.1686,  ..., 0.1529, 0.1608, 0.1569],\n           [0.1647, 0.1608, 0.1686,  ..., 0.1569, 0.1608, 0.1569]],\n \n          [[0.2902, 0.3333, 0.3490,  ..., 0.4000, 0.4000, 0.3137],\n           [0.2824, 0.3255, 0.3569,  ..., 0.4706, 0.3294, 0.3137],\n           [0.2824, 0.3216, 0.3686,  ..., 0.4627, 0.2980, 0.2431],\n           ...,\n           [0.1725, 0.1961, 0.1608,  ..., 0.1922, 0.1765, 0.1804],\n           [0.1843, 0.1686, 0.1843,  ..., 0.1804, 0.1882, 0.1843],\n           [0.1804, 0.1765, 0.1843,  ..., 0.1843, 0.1882, 0.1843]],\n \n          [[0.2627, 0.3020, 0.2980,  ..., 0.3961, 0.4000, 0.3216],\n           [0.2549, 0.2902, 0.3098,  ..., 0.4627, 0.3412, 0.3294],\n           [0.2588, 0.2902, 0.3255,  ..., 0.4549, 0.3216, 0.2784],\n           ...,\n           [0.2157, 0.2392, 0.2039,  ..., 0.2314, 0.2157, 0.2196],\n           [0.2275, 0.2118, 0.2275,  ..., 0.2196, 0.2275, 0.2235],\n           [0.2235, 0.2196, 0.2275,  ..., 0.2235, 0.2275, 0.2235]]],\n \n \n         [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           ...,\n           [0.1569, 0.1725, 0.1569,  ..., 0.1647, 0.1529, 0.1647],\n           [0.1725, 0.1686, 0.1647,  ..., 0.1569, 0.1490, 0.1490],\n           [0.1608, 0.1608, 0.1686,  ..., 0.1529, 0.1608, 0.1529]],\n \n          [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           ...,\n           [0.1725, 0.1882, 0.1725,  ..., 0.1922, 0.1804, 0.1922],\n           [0.1882, 0.1843, 0.1804,  ..., 0.1843, 0.1765, 0.1765],\n           [0.1765, 0.1765, 0.1843,  ..., 0.1804, 0.1882, 0.1804]],\n \n          [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n           ...,\n           [0.2157, 0.2314, 0.2157,  ..., 0.2314, 0.2196, 0.2314],\n           [0.2314, 0.2275, 0.2235,  ..., 0.2235, 0.2157, 0.2157],\n           [0.2196, 0.2196, 0.2275,  ..., 0.2196, 0.2275, 0.2196]]]]))"},"metadata":{}}]},{"cell_type":"markdown","source":"For the pytorch model, timm is implemented and this model uses the efficientnet_b0 model since it is a relatively fast training model. Setting pretrained to true means the model weights which are the values within the model's layers that are adjusted during training, have been adjusted from the imagenet dataset. The default feature size of the efficientnet model which is 1280 needs to match the class size of the model im creating. Additionally, the efficientnet model has an extra layer that needs to be removed as a result of needing to match the feature size with num_classes.","metadata":{}},{"cell_type":"code","source":"#create class that imports from the neural network module of pytorch\nclass VirusClassifier(nn.Module):\n    def __init__(self, num_classes = 4):\n        #Initialize object with parent class using super()\n        super(VirusClassifier, self).__init__()\n        # Defining the model\n        self.base_model = timm.create_model('efficientnet_b0', pretrained=True)\n        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n\n        enet_out_size = 1280\n        # Make a classifier to adjust enet_out size to match class size\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(enet_out_size, num_classes)\n        )\n    \n    def forward(self, x):\n        # Connect parts of the defined model and returning output\n        x = self.features(x)\n        output = self.classifier(x)\n        return output\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.963792Z","iopub.execute_input":"2023-12-03T14:26:56.964079Z","iopub.status.idle":"2023-12-03T14:26:56.971800Z","shell.execute_reply.started":"2023-12-03T14:26:56.964044Z","shell.execute_reply":"2023-12-03T14:26:56.970913Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#model is object/instance of VirusClassifier\n#test and view model details\nmodel = VirusClassifier(num_classes = 4)\nprint(str(model)[:1000])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:56.974592Z","iopub.execute_input":"2023-12-03T14:26:56.974905Z","iopub.status.idle":"2023-12-03T14:26:58.672914Z","shell.execute_reply.started":"2023-12-03T14:26:56.974855Z","shell.execute_reply":"2023-12-03T14:26:58.672036Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2af4c85a4cb4513acf0d89761a827a9"}},"metadata":{}},{"name":"stdout","text":"VirusClassifier(\n  (base_model): EfficientNet(\n    (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNormAct2d(\n      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n      (drop): Identity()\n      (act): SiLU(inplace=True)\n    )\n    (blocks): Sequential(\n      (0): Sequential(\n        (0): DepthwiseSeparableConv(\n          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (bn1): BatchNormAct2d(\n            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pw): Conv2d(32, 16, kernel_\n","output_type":"stream"}]},{"cell_type":"code","source":"#testing forward model by calling model with ex. images to see if batch size and class size are accurate\ntest_out = model(images)\ntest_out.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:58.674042Z","iopub.execute_input":"2023-12-03T14:26:58.674323Z","iopub.status.idle":"2023-12-03T14:26:59.153988Z","shell.execute_reply.started":"2023-12-03T14:26:58.674299Z","shell.execute_reply":"2023-12-03T14:26:59.153058Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([10, 4])"},"metadata":{}}]},{"cell_type":"markdown","source":"Before beginning the training loop, the datasets are set up so datasets are made from the path and dataloaders are made for each set. ","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\ntrain_folder = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/train'\nvalid_folder = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/valid'\ntest_folder = '/kaggle/input/virusmodeldataset/Data_Sources/Dataset_images/test'\n\ntrain_dataset = VirusDataset(train_folder, transform=transform)\nvalid_dataset = VirusDataset(valid_folder, transform=transform)\ntest_dataset = VirusDataset(test_folder, transform=transform)\n\n#only want shuffling when model is training, validation and test are there just for that, to validate and test the training\ntrain_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=10, shuffle=False)\ntest_loader = DataLoader(valid_dataset, batch_size=10, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:59.155215Z","iopub.execute_input":"2023-12-03T14:26:59.155500Z","iopub.status.idle":"2023-12-03T14:26:59.251502Z","shell.execute_reply.started":"2023-12-03T14:26:59.155475Z","shell.execute_reply":"2023-12-03T14:26:59.250834Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#set to train with 5 runs through entire dataset\nnum_epochs = 5\ntrain_losses, valid_losses = [], []\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = VirusClassifier(num_classes = 4)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.001)\n\nfor epoch in range(num_epochs):\n    model.train()\n    #running_loss will accumulate total loss across batches per epoch\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() + images.size(0)\n    #after one loop of epoch complete, store training loss\n    #training loss is avg loss per data point in data set per epoch\n    train_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n    \n    #validation, change model from training to validation\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n    valid_loss = running_loss / len(valid_loader.dataset)\n    valid_losses.append(valid_loss)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {valid_loss}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:26:59.252426Z","iopub.execute_input":"2023-12-03T14:26:59.252682Z","iopub.status.idle":"2023-12-03T14:27:16.879478Z","shell.execute_reply.started":"2023-12-03T14:26:59.252659Z","shell.execute_reply":"2023-12-03T14:27:16.878541Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/5 - Train loss: 1.1287650406360625, Validation loss: 1.1345195770263672\nEpoch 2/5 - Train loss: 1.0436465844511986, Validation loss: 1.010746955871582\nEpoch 3/5 - Train loss: 1.013534390926361, Validation loss: 0.9933407306671143\nEpoch 4/5 - Train loss: 1.0049243174493312, Validation loss: 0.9730596542358398\nEpoch 5/5 - Train loss: 1.0018904163502156, Validation loss: 0.9559248685836792\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As the model trains through the epochs, the training loss and validation loss is shown per epoch using the cross entropy loss criterion, which measures difference between predicted and actual probabilities. I am using cross entropy loss since my data has 4 classes and thus is a multi-class classification. \n\nThe loss values are very high, and this could be due to many reasons. Primarily, my dataset was a test dataset that I created myself and is extremely small. Additionally, the images are very noisy. The model may be doing something such as predicting incorrectly but with high confidence, leading to a higher loss. ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport numpy as np\n\n#In order to measure model accuracy, evaluated pre\n# Evaluation mode\nmodel.eval()\n\n#Variables to compute f1 score using f1_score function from sklearn.metrics\npredicted_labels = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for images, labels in valid_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        predicted_labels.extend(predicted.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n\n\nf1 = f1_score(true_labels, predicted_labels, average='weighted')\n\nprint(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:53:02.639208Z","iopub.execute_input":"2023-12-03T14:53:02.640032Z","iopub.status.idle":"2023-12-03T14:53:02.725707Z","shell.execute_reply.started":"2023-12-03T14:53:02.639995Z","shell.execute_reply":"2023-12-03T14:53:02.724748Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"F1 Score: 0.5666666666666667\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The F1 score supports my prediction of the model having low accuracy due to data sample size and image quality. For future projects, I can implement a very similar paradigm that is common through a lot of pytorch models but use a dataset with less noisy images and a much larger dataset for a higher f1 score - a higher model accuracy. ","metadata":{}}]}